# TS| 技术分享: 2次线上问题的排查与小结

下面的内容基于问题解决过程中记录的笔记, 事后从「上帝视角」再来回顾, 能对问题了解得更透彻: 问题是什么? 问题从哪里来? 问题到哪里去?

## 问题一: 服务缓存

1. 服务员方和业务方没有沟通清楚, 导致业务方使用了未加缓存限制的后台统计接口[^1], 而不是控量接口 -> 服务方系统, 卒
2. 服务方在定位问题时, 没想到是上面这个问题[^2], 看 nginx access log 时只注意了 status, 误以为是 **缓存穿透**, 将原接口中的缓存设置为永久, 另起了一个脚本来更新缓存 -> 服务方埋了一个雷[^3]
3. 服务方和业务方沟通后发现是调用错了接口, 于是反省自己 debug 问题的路径有严重问题(1. nginx access log 没仔细看; 2. mysql show full processlist 中的完整 sql)[^4], 并清理了缓存穿透逻辑的代码 -> 服务方继续埋雷[^5]
4. 业务方检查到业务上有问题, 定位到服务方的控量接口异常, 服务方发现是**缓存穿透**的代码执行过一次, 设置了永久缓存[^6] -> 服务方暴雷一
5. 服务方简单修复缓存, 加上5s缓存, 然后出现了 **缓存穿透** -> 服务方暴雷二, 系统卒[^7]
6. 服务方修改接口代码开启临时挡板, 但是 fpm 进程(200个)没有释放, 数据库中 show processlist 还处于 sending data, 代码并没有生效 -> 服务方抓狂中, 修改了代码居然不生效[^8]
7. 服务方重启服务(nginx php-fpm), 清理数据库中的 sql, 并设置缓存到 30s(服务方又天真了) -> 没彻底解决问题, 服务方系统, 卒
8. 业务方关闭控量接口, 服务方关闭服务, 清理数据库中的sql, 添加缓存穿透逻辑, 重新打开服务, 业务系统重新打开控量接口 -> 服务方写事故报告中

## 问题二: nginx 出现大量499


1. 确认监控数据
- 云服务监控: ecs mysql -> 无异常
- 日志: nginx(access->499可以报警出来[^9] error) fpm(error) 应用日志->所有error都会报警出来
2. 优化
- 增加fpm进程数 200->600
- 实时写改缓存后写: 请求进来后直接返回, 数据push到redis队列, 每分钟清队列批量插入数据库
- 可能是LB的锅: https://stackoverflow.com/questions/15613452/nginx-issues-http-499-error-after-60-seconds-despite-config-php-and-aws
上面的优化都没有完全解决, 报499的频率降低[^9]
3. 原因分析
- 只有来自h5的接口会出现499 -> 499复现的一种方式, 在浏览器中按住f5一直刷新, 那么上一个请求就会返回499
- 添加 `proxy_ignore_client_abort on;` -> 499情况下, fpm依旧会处理请求[^11]

[^1]: 一是因为后台统计接口因为会设置各种不同的查询条件, 二是因为后台的接口只在内部使用, 访问量很小
[^2]: 沟通问题, 而非技术问题, 事后注
[^3]: 并没有定位到问题, 所以这一步修改只做了一半就继续去定位问题去了
[^4]: 知识越多并且越熟悉, 定位问题的路径就越多, 但是有了这么多路径却没有定位到问题, 确实要好好反省
[^5]: 清理写到一半的代码是好习惯 -- clean code
[^6]: 清理遗留代码导致的问题, 没有注意到数据层的改动
[^7]: 原始业务的数据库查询需要耗时 200ms, qps只有5, 而业务当时的 qps 超过100
[^8]: 很多程序员可能都遇到过这样的「见鬼时候」, 经验老道一点的 coder 会脱口而出「肯定有缓存」
[^9]: 通过日志服务添加报警会灵活很多, 也能做到一定程度的解耦; 新增的报警也可以用来确定问题是否解决
[^10]: 因为每次优化都导致报错频率有所降低, 导致产生「问题已解决的错觉」
[^11]: 499还有一个原因是 fpm 长时间没有返回, 比如上面 fpm 进程因为数据库查询没有返回还在执行中