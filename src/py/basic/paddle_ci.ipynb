{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 中文分词需要jieba\n",
    "# 词云绘制需要wordcloud\n",
    "# 可视化展示中需要的中文字体\n",
    "# 网上公开资源中找一个中文停用词表\n",
    "# 根据分词结果自己制作新增词表\n",
    "# 准备一张词云背景图（附加项，不做要求）\n",
    "\n",
    "# Linux系统默认字体文件路径\n",
    "# !ls /usr/share/fonts/\n",
    "# 查看系统可用的ttf格式中文字体\n",
    "# !fc-list :lang=zh | grep \".ttf\"\n",
    "# !wget https://mydueros.cdn.bcebos.com/font/simhei.ttf # 下载中文字体\n",
    "# #创建字体目录fonts\n",
    "# !mkdir .fonts\n",
    "# # 复制字体文件到该路径\n",
    "# !cp simhei.ttf .fonts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import requests\n",
    "import json\n",
    "import re #正则匹配\n",
    "import time #时间处理模块\n",
    "import jieba #中文分词\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud  #绘制词云模块\n",
    "import paddlehub as hub\n",
    "\n",
    "#请求爱奇艺评论接口，返回response信息\n",
    "def getMovieinfo(url):\n",
    "    '''\n",
    "    请求爱奇艺评论接口，返回response信息\n",
    "    参数  url: 评论的url\n",
    "    :return: response信息\n",
    "    '''\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Referer\": \"http://m.iqiyi.com/v_19rqriflzg.html\",\n",
    "        \"Origin\": \"http://m.iqiyi.com\",\n",
    "        \"Host\": \"sns-comment.iqiyi.com\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,zh-TW;q=0.6\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\"\n",
    "    }\n",
    "    response = session.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    return None\n",
    "\n",
    "#解析json数据，获取评论\n",
    "def saveMovieInfoToFile(lastId,arr):\n",
    "    '''\n",
    "    解析json数据，获取评论\n",
    "    参数  lastId:最后一条评论ID  arr:存放文本的list\n",
    "    :return: 新的lastId\n",
    "    '''\n",
    "    url = \"https://sns-comment.iqiyi.com/v3/comment/get_comments.action?agent_type=118&\\\n",
    "    agent_version=9.11.5&business_type=17&content_id=15068699100&page=&page_size=10&types=time&last_id=\"\n",
    "    url += str(lastId)\n",
    "    responseTxt = getMovieinfo(url)\n",
    "    responseJson = json.loads(responseTxt)\n",
    "    comments = responseJson['data']['comments']\n",
    "    for val in comments:\n",
    "        # print(val.keys())\n",
    "        if 'content' in val.keys():\n",
    "            # print(val['content'])\n",
    "            arr.append(val['content'])\n",
    "        lastId = str(val['id'])\n",
    "    return lastId\n",
    "\n",
    "#去除文本中特殊字符\n",
    "def clear_special_char(content):\n",
    "    '''\n",
    "    正则处理特殊字符\n",
    "    参数 content:原文本\n",
    "    return: 清除后的文本\n",
    "    '''\n",
    "    s = re.sub(r\"</?(.+?)>|&nbsp;|\\t|\\r\", \"\", content)\n",
    "    s = re.sub(r\"\\n\", \" \", s)\n",
    "    s = re.sub('[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]', '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def fenci(text):\n",
    "    '''\n",
    "    利用jieba进行分词\n",
    "    参数 text:需要分词的句子或文本\n",
    "    return：分词结果\n",
    "    '''\n",
    "    jieba.load_userdict('add_words.txt') #添加自定义字典\n",
    "    seg = jieba.lcut(text, cut_all=False)\n",
    "    return seg\n",
    "\n",
    "def stopwordslist(file_path):\n",
    "    '''\n",
    "    创建停用词表\n",
    "    参数 file_path:停用词文本路径\n",
    "    return：停用词list\n",
    "    '''\n",
    "    stopwords = [line.strip() for line in open(file_path, encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def movestopwords(sentence,stopwords,counts):\n",
    "    '''\n",
    "    去除停用词,统计词频\n",
    "    参数 file_path:停用词文本路径 stopwords:停用词list counts: 词频统计结果\n",
    "    return：None\n",
    "    '''\n",
    "    out = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            if len(word) != 1:\n",
    "                counts[word] = counts.get(word,0) + 1\n",
    "    return None\n",
    "\n",
    "def drawcounts(counts,num):\n",
    "    '''\n",
    "    绘制词频统计表\n",
    "    参数 counts: 词频统计结果 num:绘制topN\n",
    "    return：none\n",
    "    '''\n",
    "    x_aixs = []\n",
    "    y_aixs = []\n",
    "    c_order = sorted(counts.items(), key=lambda x:x[1],reverse=True)\n",
    "    # print(c_order)\n",
    "    for c in c_order[:num]:\n",
    "        x_aixs.append(c[0])\n",
    "        y_aixs.append(c[1])\n",
    "\n",
    "    # 设置显示中文\n",
    "    matplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False   # 解决保存图像是负号'-'显示为方块的问题\n",
    "    plt.bar(x_aixs, y_aixs)\n",
    "    plt.title('词频统计结果')\n",
    "    plt.show()\n",
    "\n",
    "def drawcloud(word_f):\n",
    "    '''\n",
    "    根据词频绘制词云图\n",
    "    参数 word_f:统计出的词频结果\n",
    "    return：none\n",
    "    '''\n",
    "    #加载背景图片\n",
    "    cloud_mask = np.array(Image.open('cluod.png'))\n",
    "    #忽略显示的词\n",
    "    st=set([\"东西\",\"这是\"])\n",
    "    #生成wordcloud对象\n",
    "    wc = WordCloud(background_color='white',\n",
    "    mask=cloud_mask,\n",
    "    max_words=150,\n",
    "    font_path='simhei.ttf',\n",
    "    min_font_size=10,\n",
    "    max_font_size=100,\n",
    "    width=400,\n",
    "    relative_scaling=0.3,\n",
    "    stopwords=st)\n",
    "    wc.fit_words(word_f)\n",
    "    wc.to_file('pic.png')\n",
    "\n",
    "def text_detection(text,file_path):\n",
    "    '''\n",
    "    使用hub对评论进行内容分析\n",
    "    return：none\n",
    "    '''\n",
    "    porn_detection_lstm = hub.Module(name=\"porn_detection_lstm\")\n",
    "    f = open('aqy.txt', 'r',encoding='utf-8')\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 1:  #判断评论长度是否为1\n",
    "            continue\n",
    "        else:\n",
    "            test_text.append(line)\n",
    "    f.close()\n",
    "    input_dict = {\"text\": test_text}\n",
    "    results = porn_detection_lstm.detection(data=input_dict,use_gpu=True, batch_size=1)\n",
    "    # print(results)\n",
    "    for index, item in enumerate(results):\n",
    "        if float(item['porn_probs']) > 0.95 :\n",
    "            print(item['text'],':',item['porn_probs'])\n",
    "\n",
    "#评论是多分页的，得多次请求爱奇艺的评论接口才能获取多页评论,有些评论含有表情、特殊字符之类的\n",
    "#num 是页数，一页10条评论，假如爬取1000条评论，设置num=100\n",
    "if __name__ == \"__main__\":\n",
    "    num = 20\n",
    "    lastId = '0' #lastId 是接口分页id\n",
    "    arr = [] #arr是爬取的所有评论存放的数组\n",
    "    with open('aqy.txt', 'a', encoding='utf-8') as f: #写文件是追加写的\n",
    "        for i in range(num):\n",
    "            lastId = saveMovieInfoToFile(lastId,arr)\n",
    "            # print(i)\n",
    "            time.sleep(0.5)#频繁访问爱奇艺接口，偶尔出现接口连接报错情况，睡眠0.5秒，增加每次访问间隔时间\n",
    "        for item in arr:\n",
    "            Item = clear_special_char(item)\n",
    "            # print(Item)\n",
    "            if Item.strip()!='':\n",
    "                try:\n",
    "                    f.write(Item+'\\n')\n",
    "                except  Exception as e:\n",
    "                    print(\"含有特殊字符\")\n",
    "    print('共爬取评论：',len(arr))\n",
    "    f = open('aqy.txt', 'r',encoding='utf-8')\n",
    "    counts = {}\n",
    "    for line in f:\n",
    "        words = fenci(line)\n",
    "        stopwords = stopwordslist('cn_stopwords.txt')\n",
    "        movestopwords(words,stopwords,counts)\n",
    "\n",
    "    drawcounts(counts,10) #绘制top10 高频词\n",
    "    drawcloud(counts) #绘制词云\n",
    "    f.close()\n",
    "\n",
    "    '''\n",
    "    使用hub对评论进行内容分析\n",
    "    '''\n",
    "    file_path = 'aqy.txt'\n",
    "    test_text = []\n",
    "    text_detection(test_text,file_path)\n",
    "    display(Image.open('pic.png')) #显示生成的词云图像"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}